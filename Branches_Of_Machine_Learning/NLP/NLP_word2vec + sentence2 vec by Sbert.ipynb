{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc822d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim
    \n",
    "#for word2vec - lower case, stop words removal, input can be one word at a time, not good for sentence level vectorization (because it averages the word enclosed in the vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2554e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need word2vec to create embeddings previously we only knew how to create encoding of a text but embedding is better than encoding because embedding is low dimensional and embedding uses its dimensions very efficiently.\n",
    "#also word 2 vec returns error if it encounters any out of vocabulary word in the given text (the text that we need to vectorize) \n",
    "#always remove the stop words and pre process the text before applying the word vector\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd65a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "gn_vec_zip_path = \"F:/Machine Learning/Standard_Models_ML/GoogleNews-vectors-negative300.bin.gz\"  ##dont change this path or if changing remember there is forward slash\n",
    "with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
    "  with open(gn_vec_path, 'wb') as f_out:\n",
    "      shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22260fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings #This module ignores the various types of warnings generated\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import os\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "\n",
    "import time #This module is used to calculate the time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = gn_vec_path\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Number of words in vocablulary: \",len(w2v_model)) #Number of words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "775751e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exquisitely', 0.7916068434715271),\n",
       " ('elegantly', 0.7611106038093567),\n",
       " ('brilliantly', 0.7595520615577698),\n",
       " ('gorgeously', 0.7531605958938599),\n",
       " ('wonderfully', 0.7496622800827026),\n",
       " ('superbly', 0.7408496141433716),\n",
       " ('marvelously', 0.7399469017982483),\n",
       " ('Beautifully', 0.7366840839385986),\n",
       " ('impeccably', 0.728464663028717),\n",
       " ('expertly', 0.72747403383255)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(\"beautifully\")   #take care of brackets in this method we use curly brackets for vectors we use square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model[\"beauty\"] #will return the vector representation of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2745a013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=[\"king\",\"woman\"],negative=[\"man\"],topn = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73fac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc08242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are word analogies as well in word2vec model\n",
    "#In using word 2 vec always lower the case of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af2fce73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'lost', 'alot', 'of', 'money', 'from', 'slots']\n",
      "i\n",
      "lost\n",
      "alot\n",
      "of not in vocabulary\n",
      "money\n",
      "from\n",
      "slots\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# sentence = \"Can i get a cup of coffee?\"\n",
    "# sentence = \"Apple sells iOS smartphones\"\n",
    "sentence = \"I lost alot of money from slots\"\n",
    "tokens = text_to_word_sequence(sentence)\n",
    "print(tokens)\n",
    "\n",
    "sentence_embedding1 = np.zeros((300,), dtype=float)\n",
    "count = 0\n",
    "for token in tokens:\n",
    "    try:\n",
    "        sentence_embedding1 += w2v_model[token]\n",
    "        print(token)\n",
    "        count += 1\n",
    "    except:\n",
    "        print(token, \"not in vocabulary\")\n",
    "\n",
    "sentence_embedding1 /= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f719b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18d36b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Transformers to convert a sentence into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e076449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "178f2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "    \"It took him a month to finish the meal.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b39e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "026d54f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3308892 , 0.72192585, 0.1747551 , 0.44709665, 0.5548364 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(\n",
    "    [sentence_embeddings[0]],\n",
    "    sentence_embeddings[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subword tokenization only encoding (by bpe, wlv,wpc..)\n",
    "#word level tokenization + embedding (use word2vec or Glove Vectors)\n",
    "#sentence level tokenization +embedding +better /effective on sentence level  (use SBert )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
