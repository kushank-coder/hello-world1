#Basic NLP Processing

import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator


#Removing Stop Words and other unnecessary things

	1) nltk.download('stopwords')                 # download the stopwords from NLTK  
	2) Basic cleaning of removing hyperlinks and #
		import re                                  # library for regular expression operations
		import string                              # for string operations

		from nltk.stem import PorterStemmer        # module for stemming
		from nltk.tokenize import TweetTokenizer   # module for tokenizing strings
        from nltk.stem import PorterStemmer, WordNetLemmatizer  # Stemming and Lemmatizer


		#find and replace (sub means substitute replace some letters/ strings/ hyperlinks)
        cleaned_text = re.sub(r'[^a-zA-Z\s]', '', text)      #removes punctuation, numbers and special characters
        tweet2 = re.sub(r'https?://[^\s\n\r]+', '', tweet2)  #removes hyperlinks
        tweet2 = re.sub(r'#', '', tweet2)                    #removes hash

        #removing stop_words from a list of stop words provided by nltk

        from nltk.corpus import stopwords          # module for stop words REMOVAL
        stopwords.words('english')
        ' '.join([word for word in TEXT_TO_BE_CLEANED.split() if word not in stop_words])    

        #Lemmatization 
        lemmatizer = WordNetLemmatizer()
        ' '.join([lemmatizer.lemmatize(word) for word in TEXT.split()])

	3) Tokenize the Input text
            # tokenization should be done by spacy (because it is a model that lets the proper tokenization (split etc does not tokenize that well))

               import spacy
               nlp = spacy.load("en_core_web_sm")
               doc = nlp(text)
               list(doc) --> this will give the individual tokens that are generated

            3.1) to get list of tokens omitting or without punctuation use the following
              [token.text for token in doc if not token.is_punct]   

            3.2) Creating token_master from a dataframe column of text
                    for i, row in X_train.iterrows():
                        doc = nlp(row["cleaned_text"])
                        list_of_tokens = [token.text for token in doc if not token.is_punct] #this is valid because this is a bag of words representation
                        tokens_master.extend(list_of_tokens)
                        tokens_master = list(set(tokens_master))


        OR #CONVERTS A text into a array of tokens #sentence is the piece of text
        def sent_to_words(sentences):
            for sentence in sentences:
                yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

 
    	4) remove stop words (in english) + remove punctuation

    	stopwords_english = stopwords.words('english') #list of stop words already in a list in english in nltk library

    	for word in tweet_tokens: # Go through every word in your tokens list
    		if (word not in stopwords_english and  # remove stopwords
    		word not in string.punctuation):  # remove punctuation
        		tweets_clean.append(word)
        #tweet_cleans is cleaned list of tokens


   #Stemming
    	1) stemmer = PorterStemmer()           # Instantiate stemming class
    	2) tweets_stem = [] 
    	3) for word in tweets_clean:
    		stem_word = stemmer.stem(word)  # stemming word
    		tweets_stem.append(stem_word)  # append to the list



####NOT IMPORTANT Below this

    #Shortcut for removing stop words and doing stemming
    	1) building frequency Matrix (stem,sum(pos freq),sum(neg_freq))
    	   from utils import process_tweet, build_freqs   
    	   freqs = build_freqs(tweets, labels) #this will preprocess all tweets + makes pairs of unique (stem,sentiment) and counts its occcurences in positive y and negative y and will make a frequency matrix
    	   data = []

			# loop through our selected words
			for word in keys:
    
    		# initialize positive and negative counts
    		pos = 0
    		neg = 0
    
    		# retrieve number of positive counts
    		if (word, 1) in freqs:
        		pos = freqs[(word, 1)]
        
    		# retrieve number of negative counts
    		if (word, 0) in freqs:
        		neg = freqs[(word, 0)]
        
    		# append the word counts to the table
    		data.append([word, pos, neg])

        3) extract features from the frequencies table #there is no readymade library for this we will need to use the below function

        def extract_features(tweet, freqs, process_tweet=process_tweet):
                '''
                Input: 
                tweet: a list of words for one tweet
                freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
                Output: 
                x: a feature vector of dimension (1,3)
                '''
         word_l = process_tweet(tweet) # process_tweet tokenizes, stems, and removes stopwords
         x = np.zeros(3)  # 3 elements for [bias, positive, negative] counts 
        x[0] = 1 # bias term is set to 1
    
    
        for word in word_l: # loop through each word in the list of processed tweets
            if (freqs.get((word,1.0),-1) !=-1):   #.get() is a special function used with dictionaries...it outputs -1 if the key is not found instead of error
                x[1] += freqs[(word,1.0)]  # increment the word count for the positive label 1
            if (freqs.get((word,0.0),-1) !=-1):
                x[2] += freqs[(word,0.0)] # increment the word count for the negative label 0
    
    
        x = x[None, :]  # adding batch dimension for further processing
        assert(x.shape == (1, 3))
        return x 
    


 


