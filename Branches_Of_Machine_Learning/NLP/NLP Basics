#Basic NLP Processing

import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator


#Removing Stop Words and other unnecessary things

	1) nltk.download('stopwords')                 # download the stopwords from NLTK  
	2) Basic cleaning of removing hyperlinks and #
		import re                                  # library for regular expression operations
		import string                              # for string operations

		from nltk.corpus import stopwords          # module for stop words that come with NLTK
		from nltk.stem import PorterStemmer        # module for stemming
		from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

		#sub function helps to remove the hyperlinks or replace some letters/ strings
		tweet2 = re.sub(r'https?://[^\s\n\r]+', '', tweet2)  #removes hyperlinks
		tweet2 = re.sub(r'#', '', tweet2)                    #removes hash

	3) Tokenize the Input text
		tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)     # instantiate tokenizer class
    	tweet_tokens = tokenizer.tokenize(tweet2)                                               #tweet_tokens will be tokenized
 
    	4) remove stop words (in english) + remove punctuation

    	stopwords_english = stopwords.words('english') #list of stop words already in a list in english in nltk library

    	for word in tweet_tokens: # Go through every word in your tokens list
    		if (word not in stopwords_english and  # remove stopwords
    		word not in string.punctuation):  # remove punctuation
        		tweets_clean.append(word)
        #tweet_cleans is cleaned list of tokens


   #Stemming
    	1) stemmer = PorterStemmer()           # Instantiate stemming class
    	2) tweets_stem = [] 
    	3) for word in tweets_clean:
    		stem_word = stemmer.stem(word)  # stemming word
    		tweets_stem.append(stem_word)  # append to the list


    #Shortcut for removing stop words and doing stemming
    	1) from utils import process_tweet
    	   tweets_stem = process_tweet(tweet); #tweets stem is stemmed + stop words removed + punctuation removed + Lower Case


    	2) building frequency Matrix (stem,sum(pos freq),sum(neg_freq))
    	   from utils import process_tweet, build_freqs   
    	   freqs = build_freqs(tweets, labels) #this will preprocess all tweets + makes pairs of unique (stem,sentiment) and counts its occcurences in positive y and negative y and will make a frequency matrix
    	   data = []

			# loop through our selected words
			for word in keys:
    
    		# initialize positive and negative counts
    		pos = 0
    		neg = 0
    
    		# retrieve number of positive counts
    		if (word, 1) in freqs:
        		pos = freqs[(word, 1)]
        
    		# retrieve number of negative counts
    		if (word, 0) in freqs:
        		neg = freqs[(word, 0)]
        
    		# append the word counts to the table
    		data.append([word, pos, neg])
    


 


