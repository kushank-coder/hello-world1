#Basic NLP Processing

import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator


#Removing Stop Words and other unnecessary things

	1) nltk.download('stopwords')                 # download the stopwords from NLTK  
	2) Basic cleaning of removing hyperlinks and #
		import re                                  # library for regular expression operations
		import string                              # for string operations

		from nltk.corpus import stopwords          # module for stop words that come with NLTK
		from nltk.stem import PorterStemmer        # module for stemming
		from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

		#find and replace (sub means substitute the hyperlinks or replace some letters/ strings)
		tweet2 = re.sub(r'https?://[^\s\n\r]+', '', tweet2)  #removes hyperlinks
		tweet2 = re.sub(r'#', '', tweet2)                    #removes hash
        text_without_dots = re.sub(r'\.{3,}$', '', text_without_links_v2) # remove trailing multiple dots (generally used by people)....
        text_without_exclamation_end =  re.sub(r'!+$', '', text_without_dots)                    # removes trailing exclamation marks..

	3) Tokenize the Input text
		tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)     # instantiate tokenizer class
    	tweet_tokens = tokenizer.tokenize(tweet2)                                               #tweet_tokens will be tokenized

        OR #CONVERTS A text into a array of tokens #sentence is the piece of text
        def sent_to_words(sentences):
            for sentence in sentences:
                yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

 
    	4) remove stop words (in english) + remove punctuation

    	stopwords_english = stopwords.words('english') #list of stop words already in a list in english in nltk library

    	for word in tweet_tokens: # Go through every word in your tokens list
    		if (word not in stopwords_english and  # remove stopwords
    		word not in string.punctuation):  # remove punctuation
        		tweets_clean.append(word)
        #tweet_cleans is cleaned list of tokens


   #Stemming
    	1) stemmer = PorterStemmer()           # Instantiate stemming class
    	2) tweets_stem = [] 
    	3) for word in tweets_clean:
    		stem_word = stemmer.stem(word)  # stemming word
    		tweets_stem.append(stem_word)  # append to the list



####NOT IMPORTANT Below this

    #Shortcut for removing stop words and doing stemming
    	1) building frequency Matrix (stem,sum(pos freq),sum(neg_freq))
    	   from utils import process_tweet, build_freqs   
    	   freqs = build_freqs(tweets, labels) #this will preprocess all tweets + makes pairs of unique (stem,sentiment) and counts its occcurences in positive y and negative y and will make a frequency matrix
    	   data = []

			# loop through our selected words
			for word in keys:
    
    		# initialize positive and negative counts
    		pos = 0
    		neg = 0
    
    		# retrieve number of positive counts
    		if (word, 1) in freqs:
        		pos = freqs[(word, 1)]
        
    		# retrieve number of negative counts
    		if (word, 0) in freqs:
        		neg = freqs[(word, 0)]
        
    		# append the word counts to the table
    		data.append([word, pos, neg])

        3) extract features from the frequencies table #there is no readymade library for this we will need to use the below function

        def extract_features(tweet, freqs, process_tweet=process_tweet):
                '''
                Input: 
                tweet: a list of words for one tweet
                freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
                Output: 
                x: a feature vector of dimension (1,3)
                '''
         word_l = process_tweet(tweet) # process_tweet tokenizes, stems, and removes stopwords
         x = np.zeros(3)  # 3 elements for [bias, positive, negative] counts 
        x[0] = 1 # bias term is set to 1
    
    
        for word in word_l: # loop through each word in the list of processed tweets
            if (freqs.get((word,1.0),-1) !=-1):   #.get() is a special function used with dictionaries...it outputs -1 if the key is not found instead of error
                x[1] += freqs[(word,1.0)]  # increment the word count for the positive label 1
            if (freqs.get((word,0.0),-1) !=-1):
                x[2] += freqs[(word,0.0)] # increment the word count for the negative label 0
    
    
        x = x[None, :]  # adding batch dimension for further processing
        assert(x.shape == (1, 3))
        return x 
    


 


