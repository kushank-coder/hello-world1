{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa8fb00",
   "metadata": {},
   "source": [
    "## Breaking of long text into small pieces is tokenization. Then we represent it in number by one hot encoding. Further for reducing dimensions we use Embedding. We can use one hot encoding or embeddings as an input into our machine learning algorithms",
    "\n",
    "### SUMMARY\n",
    "Tokenization can be done in 3 ways\n",
    "\n",
    "#1) word Tokenizer : \n",
    "\n",
    "    a) already trained (only breaks text into small case words)  (NO VECTORIZATION):- \n",
    "                        from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "                        result = text_to_word_sequence(text) \n",
    "    b) Can train our self tokenizer as well (YES VECTORIZATION):\n",
    "                        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "                        #instance of class tokenizer #always include oov_token = 'unk'\n",
    "                        t = Tokenizer(oov_token='<unk>')\n",
    "                        #Training of tokenizer docs is the list of corpus documents [doc1,doc2,doc3..] \n",
    "                        t.fit_on_texts(docs)   \n",
    "                        #will give one hot encoding of the model fitted on list of docs\n",
    "                        t.texts_to_matrix([\"please welcome Eminem\"],mode='binary')  \n",
    "                        #there are other methods as well for tokenizer class \n",
    "                        #mode could be ('freq','tfidf','count','binary' )\n",
    "                        #'freq' measures the proportion of words in a particular document\n",
    "                        \n",
    "                        t.texts_to_sequences(['what a dirty place']) \n",
    "                        #this method will be use if we are training any sequence models like RNN\n",
    "                        \n",
    "#2) Sub word Tokenizer\n",
    "                    from tokenizers import Tokenizer\n",
    "                    from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "                    from tokenizers.trainers import BpeTrainer, WordLevelTrainer, WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "                    ## a pretokenizer to segment the text into words ie at max a word will be given as the tokenizer\n",
    "                    from tokenizers.pre_tokenizers import Whitespace\n",
    "                    \n",
    "                    # 1st RUN \"prepare_tokenizer_trainer\" FUNCTION in the below code\n",
    "                    #2nd run \"train_tokenizer\" function in the below code\n",
    "                    #3rd run \"tokenize\" function defined in the below code\n",
    "                    #4th Train the tokenizer\n",
    "                    trained_tokenizer = train_tokenizer(files, alg) #where alg =  ['WLV', 'BPE', 'UNI', 'WPC']\n",
    "                    #5th create tokens of any new text\n",
    "                    output = tokenize(text, trained_tokenizer) #text = 'any text we require to tokenize'\n",
    "                    #6th return vector reporesentation of the new text\n",
    "                    print(output.ids) #this would be output for RNN model we can create one hot encoding as well from this\n",
    "                    \n",
    "\n",
    "\n",
    "#3) Character Tokenizer (Not generally used (so not in this python code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a13ebbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detailed codes and their output are in the below text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ffa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "# Splits at space \n",
    "\n",
    "#very crude way of tokenize because it includes , . and so on\n",
    "text.split() \n",
    "\n",
    "#sentence tokenizer\n",
    "text.split(\". \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4617c6",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "Tokenizer could be trained or there are readymade tokenizers (which need not be trained) as well in keras.\n",
    "tokenizer is required just call the class and then convert into tokens and then vectorization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6f07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['founded', 'in', '2002', 'spacex’s', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi', 'planet', 'species', 'by', 'building', 'a', 'self', 'sustaining', 'city', 'on', 'mars', 'in', '2008', 'spacex’s', 'falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid', 'fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'earth']\n"
     ]
    }
   ],
   "source": [
    "#Keras lowers the case of all the alphabets before tokenizing them. A very good tokenizer\n",
    "#Readymade tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedfd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tokenizer class    #very important and main method for the CREATING TOKENS\n",
    "#training of word tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['How are you doing Kushank?',\n",
    "'My name is Gurjot Singh Kushank',\n",
    "'Nice to meet you',\n",
    "'What is your name Kushank Kushank?',\n",
    "'what a nice place to meet!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer(oov_token='<unk>')\n",
    "# fit the tokenizer on the documents  ie THe model is getting trained by using t.fit_on_texts(docs)\n",
    "t.fit_on_texts(docs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410db4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('how', 1), ('are', 1), ('you', 2), ('doing', 1), ('kushank', 4), ('my', 1), ('name', 2), ('is', 2), ('gurjot', 1), ('singh', 1), ('nice', 2), ('to', 2), ('meet', 2), ('what', 2), ('your', 1), ('a', 1), ('place', 1)])\n",
      "5\n",
      "{'<unk>': 1, 'kushank': 2, 'you': 3, 'name': 4, 'is': 5, 'nice': 6, 'to': 7, 'meet': 8, 'what': 9, 'how': 10, 'are': 11, 'doing': 12, 'my': 13, 'gurjot': 14, 'singh': 15, 'your': 16, 'a': 17, 'place': 18}\n",
      "defaultdict(<class 'int'>, {'you': 2, 'are': 1, 'how': 1, 'kushank': 3, 'doing': 1, 'gurjot': 1, 'name': 2, 'my': 1, 'is': 2, 'singh': 1, 'meet': 2, 'to': 2, 'nice': 2, 'your': 1, 'what': 2, 'place': 1, 'a': 1})\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs) #counts whether a word appears in a document or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6f31bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<unk>',\n",
       " 2: 'kushank',\n",
       " 3: 'you',\n",
       " 4: 'name',\n",
       " 5: 'is',\n",
       " 6: 'nice',\n",
       " 7: 'to',\n",
       " 8: 'meet',\n",
       " 9: 'what',\n",
       " 10: 'how',\n",
       " 11: 'are',\n",
       " 12: 'doing',\n",
       " 13: 'my',\n",
       " 14: 'gurjot',\n",
       " 15: 'singh',\n",
       " 16: 'your',\n",
       " 17: 'a',\n",
       " 18: 'place'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d69f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding of the corpus\n",
    "print(t.texts_to_matrix(docs,mode='binary'))  #this has 19 columns (But we have 18 indexes (this is because 1st column is the column of padding) #word vectorizer) Kind of a one hot encoding\n",
    "#mode could be ('freq','tfidf','count','binary' ) #'freq' measures the proportion of words in a particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e672120a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 17, 1, 18]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_to_sequences returns the index of the words in the sentence\n",
    "t.texts_to_sequences(['what a dirty place'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff9eef",
   "metadata": {},
   "source": [
    "## Sub Word Tokenizer \n",
    " 1st train the sub word tokenizer on complete corpora and then tokenise and then vectorization  of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eafd9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "## a pretokenizer to segment the text into words ie at max a word will be given as the tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c59de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    ‘WLV’ - Word Level Algorithm\n",
    "    ‘WPC’ - WordPiece Algorithm\n",
    "    ‘BPE’ - Byte Pair Encoding\n",
    "    ‘UNI’ - Unigram\n",
    "    2 classes are called one is tokenizer + one is trainer (subword level tokenization is like fitting a model and then\n",
    "    identifying tokens of the given text and then their one hot encoding)\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens, vocab_size = 40000) #can changes the total vocabulary size as well\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens, vocab_size = 40000)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens, vocab_size = 40000)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens, vocab_size = 40000)\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4321e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(file, alg='WLV'):\n",
    "    \"\"\"\n",
    "    Takes the file and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "    tokenizer.train(file, trainer) # training the tokenzier\n",
    "    tokenizer.save(f\"./tokenizer-trained-{alg}.json\")\n",
    "    tokenizer = Tokenizer.from_file(f\"./tokenizer-trained-{alg}.json\")\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize(input_string, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string using the tokenizer provided.\n",
    "    \"\"\"\n",
    "    output = tokenizer.encode(input_string)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da62ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Using vocabulary from ['Tokenizer_data.txt']=======\n",
      "---- WLV ----\n",
      "['This', 'is', 'a', 'deep', 'learning', '<UNK>', '<UNK>', '.', '<UNK>', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', '<UNK>', '<UNK>', '.', 'We', 'will', 'be', 'comparing', 'the', '<UNK>', 'generated', 'by', 'each', '<UNK>', 'model', '.', '<UNK>', 'much', '<UNK>'] -> 35\n",
      "---- BPE ----\n",
      "['This', 'is', 'a', 'deep', 'learning', 'to', 'ken', 'ization', 't', 'ut', 'or', 'ial', '.', 'T', 'ok', 'en', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', 'k', 'ens', 'generated', 'by', 'each', 'to', 'ken', 'ization', 'model', '.', 'Ex', 'c', 'ited', 'much', '?', '!', '<UNK>'] -> 55\n",
      "---- UNI ----\n",
      "['Thi', 's', 'is', 'a', 'deep', 'learn', 'ing', 'to', 'ken', 'iz', 'ation', 't', 'u', 'to', 'rial', '.', 'To', 'ken', 'iz', 'ation', 'is', 'the', 'fir', 's', 't', 'step', 'in', 'a', 'deep', 'learn', 'ing', 'N', 'L', 'P', 'pi', 'pe', 'line', '.', 'We', 'will', 'be', 'compar', 'ing', 'the', 'to', 'ken', 's', 'generat', 'ed', 'by', 'each', 'to', 'ken', 'iz', 'ation', 'model', '.', 'E', 'x', 'c', 'it', 'ed', 'm', 'uch', '?', '!', '😍'] -> 67\n",
      "---- WPC ----\n",
      "['This', 'is', 'a', 'deep', 'learning', 'to', '##ken', '##ization', 't', '##ut', '##oria', '##l', '.', 'To', '##ken', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'N', '##L', '##P', 'pip', '##el', '##ine', '.', 'We', 'will', 'be', 'comparing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'model', '.', 'Ex', '##ci', '##ted', 'much', '<UNK>'] -> 52\n",
      "Encoding(num_tokens=52, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "#Note we generally need a text file to train the subword tokenizer (it might be possible to train from variable (BUT this code gives errors on that))\n",
    "small_file = ['Tokenizer_data.txt']\n",
    "#'Tokenizer_data.txt'\n",
    "tokens_dict = {}\n",
    "\n",
    "for files in [small_file]:\n",
    "    print(f\"========Using vocabulary from {files}=======\")\n",
    "    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
    "        trained_tokenizer = train_tokenizer(files, alg)\n",
    "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😍\"\n",
    "        output = tokenize(input_string, trained_tokenizer)\n",
    "        tokens_dict[alg] = output.tokens\n",
    "        print(\"----\", alg, \"----\")\n",
    "        print(output.tokens, \"->\", len(output.tokens))\n",
    "#ids is the representation of text into numbers (ie it gives index of the tokens)\n",
    "print(output.ids)\n",
    "# .type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5da3ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Encoding.token_to_sequence(self, token_index)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start from 57 minutes Text Representation in nlp\n",
    "output.token_to_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e1fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
