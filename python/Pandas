#Pandas are equivalent to excel
### Creating a Series

You can convert a list,numpy array, or dictionary to a Series:

** Using Lists**

pd.Series(data=my_list)

# Converting NumPy Arrays into series

pd.Series(numpy_array)

#Dictionary gets easily converted into series.         

pd.Series(dictionary_name)                       

### Data in a Series

pd.Series(data=labels)

# Even functions (although unlikely that you will use this)
pd.Series([sum,print,len])

## Series can be easily accessible by the index (NO use of .loc required)


#Converts a Series data to dictionary data type
  s.to_dict()                                                     



# DataFrames

from numpy.random import randn
np.random.seed(101)

## NOTE: there is only one way to manually make a dataframe ie pd.DataFrame(data,indexes,column names)
##       other ways are we can import from csv in dataframe
NOTE : even dictionaries can be used to create dataframes in pandas ..the point is key in dictionary forms the column names and index is by default 0 1 2 etc.

## NOTE: we always need to use inplace = True  for actually bringing any change in the dataframe

# type of data object
type(df['W'])

** Removing Columns**

df.drop('new',axis=1)

# Not inplace unless specified!

df.drop('new',axis=1,inplace=True)                                            #inplace= True is must to actually edit the data frame "df"

# Slicing directly in pandas
df[:2]                                                                      #will return first 2 rows of the dataframe (ie 1st 2 entries)

#deleting any row in the dataframe

df.drop(df[df["1st"]=="K"].index,inplace= True)                             #we just 1st need to slice and then use .index to easily any row or conditioned row from dataframe


#Accessing the rows and columns of data frame

df.loc['A']

Or select based off of position instead of label 

df.iloc[2]
#NOTE: the method .iloc or .loc could only be used to access the already data dimensions to add a row we need to use direct columns and not loc procedure

### Conditional Selection

An important feature of pandas is conditional selection using bracket notation, very similar to numpy:

## Note : for conditional selection there is NO USE OF  ".iloc"  .we need to use iloc to slice the columns but conditional selection can be done directly and it filters the series of row values first. 
df

df[df>0]

df[df['W']>0]

For two conditions you can use | and & with parenthesis:

df[(df['W']>0) & (df['Y'] > 1)]                               #Note : If we use "and" operator it only takes 2 booleans only not a series of them ....whereas with "&" operator even a series of boleans can be compared
                                                                      same is true with "or" operator as well we need to use | for comparing series of booleans        
  ## More Index Details

Let's discuss some more features of indexing, including resetting the index or setting it something else. We'll also talk about index hierarchy!

df

# Reset to default 0,1...n index
df.reset_index()


# Missing Data
d = np.nan

df = pd.DataFrame({'A':[1,2,np.nan],
                  'B':[5,np.nan,np.nan],
                  'C':[1,2,3]})
                  
#Identifying Missing Values in Column
df.isnull().sum()                                        #will provide the missing values in any column

#Dropping rows or columns from a dataframe

df.dropna()
df.fillna(value='FILL VALUE')

#Appending data frame 
  pd.concat([df,y])

#Group by
  df.groupby('Company')                 #"Company" is the name of the column by which we group by
                                        # use Aggregate function over and above this (like sum(), mean() etc)

# Joins (merging)
check = df1.merge(df2,left_on=["value"],right_on=["number_value"],how="left")             #suffixes removes the extra column-name name 

#unique method
df['col2'].unique()                                                            #gives distinct elements

# proc freq
df['col2'].value_counts()
 
 # using operations computing another column 
                                                                                #the column on which the operation is to be done is col1 .it is mentioned in the syntax
 df['col1'].apply(times2)                                                       #times 2 is the required function  
 
 df['col1'].apply(lambda x: x^2)                                                 #Dynamic calculation of the column for quick application  x (each col1 entry) is input and output is x^2 
 
 # sorting
 df1.sort_values(by = ['frequency', 'index'], ascending = [False, True])                                                        #inplace=False by default....for sorting the data you need inplace =True
 
 #converting categorical values to dummies
 pd.get_dummies(train['col1'],drop_first=True)                                    #col1 is basically the column which we want to convert into 0 or 1 ie create dummies for them
 
 
 
 
 #pivot table

data = df.groupby("column_to_be_kept_in_rows")
pivot = data.agg({'userId': pd.Series.nunique , "rating": ["sum","count"]})      #using .agg function solves this problem....we can apply 
                                                                                #different and multiple functions to each of the column ..even select less 




# reading csv
df = pd.read_csv('example')

# writing to csv 
df.to_csv('example',index=False)                                                               #here index should equal to False (otherwise indexes will come as a column in csv

# reading excel
pd.read_excel('Excel_Sample.xlsx',sheetname='Sheet1')

# writing to excel
df.to_excel('Excel_Sample.xlsx',sheet_name='Sheet1')

#reading from html links                                                                    
df = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')                 #Extracting data from websites (internet)    #there could be multiple data frames (initially df would be a list extract from list the dataframe

#proc contents
df.info()

#proc sort data = data1 nodupkey;
df.sort_values(by='col2',inplace=True)                                                                  #This step will sort the data
vs2=vs.drop_duplicates(subset=["COLUMN NAMES On which to find duplicates"])                             #This will delete duplucate entries.

# getting dummies values for categorical_values
pd.get_dummies(train['Sex'],drop_first=True)                                                            #used in modelling  


#Selecting null values
data.isnull()

#Selecting specific columns 
data[["Pclass","Cabin"]]                                                                            #simply use double brackets 

# renaming columns in the dataframe
data_v2.rename(columns = {'Q':'Q_v1',"S":"S_v1"}, inplace = True)

#counting distinct elements in a column in dataframe
ratings['movieId'].nunique()                                                                      #will tell distinct counts in the dataframe ratings

#text to columns in pandas
movies["genres"].str.split('|', expand=True)                                                      #splits 1 column into mutiple columns based on the delimiter  


#substring a column in  pandas

dataframe["column1"].str[1:3]                                                                      #this substring slices from 2nd letter and 3rd letter






#Proc SQL Queries

from pandasql import sqldf 
mysql = lambda q: sqldf(q, globals())
query = '''
SELECT Customer_Region, sum(Churn) as churn,sum(Customer_Count) as count,100*sum(churn)/sum(customer_count) as churn_rate
FROM df
group by 1;
'''
mysql(query)                                                                                          #Note: the pandassql does integer division in the query so first multiply / divide by scalar.  


#add rows in an existing dataframe
df.loc[len(df.index)] = [check2,i]                     #adds a row at the end of the dataframe


#Save a dataframe  (pickle)

import pickle
with open('links_Meta.pickle', 'wb') as f:
    pickle.dump(links, f)

#extract from pickle data
import pickle
with open('links_Meta.pickle', 'rb') as f:
    mynewlist = pickle.load(f)
