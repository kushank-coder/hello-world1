86543#Train test split of data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)

#Principal Components Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=2)                   #always try to have explained variance to be greater than 95% atleast

#####
Plotting positive and negative examples
            Steps: from matplotlib import pyplot
                    dummy = X[["Age_v2","Fare"]]                  # CHANGE Select 2 columns to show decision boundary
                    dummy_v2 = dummy.to_numpy()
                      fig = pyplot.figure()
                      pos = y == 1
                      neg = y == 0
                      pyplot.plot(dummy_v2[pos, 0], dummy_v2[pos, 1], 'k*', lw=2, ms=10)
                      pyplot.plot(dummy_v2[neg, 0], dummy_v2[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)
                      pyplot.xlabel('Age_v2')                        # CHANGE Name of 1st column
                      pyplot.ylabel('Fare')                          # CHANGE Name of 2nd column
                      pyplot.legend(['Survived', 'Not Survived'])
                       pass


# precision, recall and F1 score
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

#confusion_matrix
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,predictions))


#Linear Regression
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)


#Logistic Regression
logmodel = LogisticRegression()                    #Change C (opposite of regularization) and maxiter to increase the iterations
logmodel.fit(X_train,y_train)

            Training accuracy
            logmodel.score(X_train, y_train)

