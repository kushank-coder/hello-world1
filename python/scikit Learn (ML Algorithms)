#Train test split of data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101,stratify = y)# there is stratify option as well

#Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)

from sklearn.preprocessing import MinMaxScaler   #min-max scaler
scaler = MinMaxScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)



#Correlation coefficient
cor = df.corr()
plt.figure(figsize = (16,16))
sns.heatmap(cor,annot=True)                #heatmap can be used to find missing values as well     

#Selecting Highly Correlated columns (the below method selects the first of the highly correlated variable and keeps the second

def correlation(dataset, threshold):                                              

    col_corr = set()  # Set of all the names of correlated columns                       # it will remove the first feature that is correlated with anything other feature
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr


#Principal Components Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=2)                   #always try to have explained variance to be greater than 95% atleast

#####
Plotting positive and negative examples
            Steps: from matplotlib import pyplot
                    dummy = X[["Age_v2","Fare"]]                  # CHANGE Select 2 columns to show decision boundary
                    dummy_v2 = dummy.to_numpy()
                      fig = pyplot.figure()
                      pos = y == 1
                      neg = y == 0
                      pyplot.plot(dummy_v2[pos, 0], dummy_v2[pos, 1], 'k*', lw=2, ms=10)
                      pyplot.plot(dummy_v2[neg, 0], dummy_v2[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)
                      pyplot.xlabel('Age_v2')                        # CHANGE Name of 1st column
                      pyplot.ylabel('Fare')                          # CHANGE Name of 2nd column
                      pyplot.legend(['Survived', 'Not Survived'])
                       pass

#Learning Curve
            from sklearn.model_selection import learning_curve
            train_sizes, train_scores, test_scores = learning_curve(LogisticRegression(fit_intercept=True,max_iter=400,C = 1000), X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))

            train_miss_Classification = 1- train_scores
            test_miss_Classification = 1- test_scores

            train_mean = np.mean(train_miss_Classification, axis=1)
            train_std = np.std(train_miss_Classification, axis=1)
            test_mean = np.mean(test_miss_Classification, axis=1)
            test_std = np.std(test_miss_Classification, axis=1)
            #
            # Plot the learning curve
            #
            plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Train Error')
            plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')
            plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='CValidation error')
            plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')
            plt.title('Learning Curve')
            plt.xlabel('Training Data Size')
            plt.ylabel('Miss Classification Error')
            plt.grid()
            plt.legend(loc='lower right')
            plt.show()
            
            
            
      2ND METHOD (Very helpful in keras neural network 
      
            # store the initial random weights
            initial_weights = model.get_weights()
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3)
            train_sizes = (len(X_train) * np.linspace(0.1, 0.999, 100)).astype(int)
          
            train_scores = []
            test_scores = []

            for train_size in train_sizes:
                X_train_frac, _, y_train_frac, _ = \
                train_test_split(X_train, y_train, train_size=train_size)
    
                # at each iteration reset the weights of the model
                # to the initial random weights
                  model.set_weights(initial_weights)
    
                        h = model.fit(X_train_frac, y_train_frac,
                                    verbose=0,
                                    epochs=300,
                                    callbacks=[EarlyStopping(monitor='loss', patience=1)])

                        r = model.evaluate(X_train_frac, y_train_frac, verbose=0)
                        train_scores.append(r[-1])
    
                        e = model.evaluate(X_test, y_test, verbose=0)
                        test_scores.append(e[-1])
    
                        plt.plot(train_sizes, 1-np.array(train_scores), 'o-', label="Training score")
                        plt.plot(train_sizes, 1-np.array(test_scores), 'o-', label="Test score")
                        plt.legend(loc="best")

# precision, recall and F1 score
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

#accuracy
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions)

#roc_auc score
from sklearn.metrics import roc_auc_score # apply this on model to get AUC of ROC



#confusion_matrix
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,predictions).T)   #Just take .T IE TRANPOSE to convert the matrix in your confusion matrix (in sklearn it is a little different how the confucious matrix is made)


#Linear Regression
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)
prediction  = lm.predict(X_test)
r2_score(Y_test,prediction)

#L2 Regularization + Linear Regression
from sklearn.linear_model import Ridge
ridg = Ridge(alpha=1.0)                  #can change the regularization parameter (similarly we could use LASSO Regression as well)
ridg.fit(X, y)
prediction  = ridg.predict(X_test)
r2_score(Y_test,prediction)

model = LassoRegression(iterations=1000, learning_rate=0.01, l1_penalty=500) # Lasso REGRESSION (L1 regularization) 


#Logistic Regression
from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()                    #Change C (opposite of regularization) and maxiter to increase the iterations
                                                    # For a multi_class problem, if multi_class is set to be “multinomial” the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e. calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.                                            

logmodel = LogisticRegression(penalty=None,multi_class='ovr', solver='liblinear') #for multi class classification (one vs all 
                                                                                    method)    liblinear is support library
logmodel = LogisticRegression(multi_class='multinomial') #for multi class classification using softmax function just like it happens in neural networks (there are 3 columns for multiclass (3 classes) )

logmodel.fit(X_train,y_train)

            Training accuracy
            logmodel.score(X_train, y_train)


# Neural Network

                                                 - Always convert the matrices of X and y into numpy arrays before inputting in the keras or tensorflow

                                  Step1)
                                    from tensorflow.keras.models import Sequential
                                    from tensorflow.keras.layers import Dense, Activation

                                    model = Sequential()

                                    model.add(Dense(2))
                                    model.add(Dense(2))
                                    model.add(Dense(2))

                                    Note: Choosing an optimizer and loss Keep in mind what kind of problem you are trying to solve:

                                          # For a multi-class classification problem
                                            model.compile(optimizer='rmsprop',
                                                              loss='categorical_crossentropy',
                                                                                                metrics=['accuracy'])
                
                                          # For a binary classification problem
                                                model.compile(optimizer='rmsprop',
                                                                        loss='binary_crossentropy',
                                                                                                metrics=['accuracy'])

                                         # For a mean squared error regression problem
                                            model.compile(optimizer='rmsprop',
                                                                         loss='mse')
                                    Step 2)
                                    model.fit(X_train,y_train,epochs=250)
                                    model.history.history     #just for getting the history as a dataframe
                                    
                                    Step 3) Evaluation
                                    training_score = model.evaluate(X_train,y_train,verbose=0)
                                    test_score = model.evaluate(X_test,y_test,verbose=0)




# Decision Trees                
(
Note: 1)feature scaling not required in decison trees because principal is information gain and no comparison of one variable wrt another variable
2) Gains Ratio in ML course( Gains / )
)

from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_train,y_train)


            #Visualization of decision trees
           from IPython.display import Image  
            from sklearn.externals.six import StringIO  
            from sklearn.tree import export_graphviz
            import pydot 

            features = list(df.columns[1:])
            features

           dot_data = StringIO()  
            export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)

            graph = pydot.graph_from_dot_data(dot_data.getvalue())  
            Image(graph[0].create_png())  



#Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train, y_train)




# Support Vector Machines

from sklearn.svm import SVC
model =SVC()
model.fit(X_train,y_train)

            param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}  #checking for the suitable parameters
            from sklearn.model_selection import GridSearchCV
            grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)
            grid.fit(X_train,y_train)


