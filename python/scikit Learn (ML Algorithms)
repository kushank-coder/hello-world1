86543#Train test split of data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)

#Principal Components Analysis
from sklearn.decomposition import PCA
pca = PCA(n_components=2)                   #always try to have explained variance to be greater than 95% atleast

#####
Plotting positive and negative examples
            Steps: from matplotlib import pyplot
                    dummy = X[["Age_v2","Fare"]]                  # CHANGE Select 2 columns to show decision boundary
                    dummy_v2 = dummy.to_numpy()
                      fig = pyplot.figure()
                      pos = y == 1
                      neg = y == 0
                      pyplot.plot(dummy_v2[pos, 0], dummy_v2[pos, 1], 'k*', lw=2, ms=10)
                      pyplot.plot(dummy_v2[neg, 0], dummy_v2[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)
                      pyplot.xlabel('Age_v2')                        # CHANGE Name of 1st column
                      pyplot.ylabel('Fare')                          # CHANGE Name of 2nd column
                      pyplot.legend(['Survived', 'Not Survived'])
                       pass

#Learning Curve
            from sklearn.model_selection import learning_curve
            train_sizes, train_scores, test_scores = learning_curve(LogisticRegression(fit_intercept=True,max_iter=400,C = 1000), X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50))

            train_miss_Classification = 1- train_scores
            test_miss_Classification = 1- test_scores

            train_mean = np.mean(train_miss_Classification, axis=1)
            train_std = np.std(train_miss_Classification, axis=1)
            test_mean = np.mean(test_miss_Classification, axis=1)
            test_std = np.std(test_miss_Classification, axis=1)
            #
            # Plot the learning curve
            #
            plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Train Error')
            plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')
            plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='CValidation error')
            plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')
            plt.title('Learning Curve')
            plt.xlabel('Training Data Size')
            plt.ylabel('Miss Classification Error')
            plt.grid()
            plt.legend(loc='lower right')
            plt.show()

# precision, recall and F1 score
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

#confusion_matrix
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,predictions))


#Linear Regression
from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)


#Logistic Regression
logmodel = LogisticRegression()                    #Change C (opposite of regularization) and maxiter to increase the iterations
logmodel.fit(X_train,y_train)

            Training accuracy
            logmodel.score(X_train, y_train)

